# üåä Fluxo Detalhado de Ingest√£o ACODE

## üìã Vis√£o Geral do Fluxo

O fluxo de ingest√£o ACODE √© um pipeline cr√≠tico que processa dados fiscais e comerciais da farmarcas atrav√©s de m√∫ltiplas etapas, garantindo integridade e disponibilidade dos dados para an√°lises de business intelligence.

```mermaid
graph TD
    subgraph "üè• Sistema ACODE Externo"
        A[fa:fa-database MySQL ACODE<br/>db-hsp-farmarcas.acode.com.br:3306]
        A1[fa:fa-table farmarcas_si_analitico_diario<br/>~50M+ registros fiscais]
        A2[fa:fa-table farmarcas_si_analitico_diario_produtos<br/>~500K registros de produtos]
    end
    
    subgraph "üîÑ Airbyte Platform v0.3.23"
        B[fa:fa-plug Source MySQL<br/>airbyte/source-mysql v1.0.21]
        C[fa:fa-cogs Connection<br/>connection_mysql_s3_acode]
        D[fa:fa-cloud Destination S3<br/>airbyte/destination-s3 v0.3.23]
    end
    
    subgraph "‚òÅÔ∏è AWS S3 Bronze Layer"
        E[fa:fa-folder farmarcas-production-bronze]
        E1[fa:fa-file bronze_acode/farmarcas_si_analitico_diario/<br/>cog_dt_ingestion=YYYY-MM-DD/]
        E2[fa:fa-file bronze_acode/farmarcas_si_analitico_diario_produtos/<br/>cog_dt_ingestion=YYYY-MM-DD/]
    end
    
    subgraph "üöÄ Airflow Orchestration"
        F[fa:fa-rocket DAG: dag_sync_connection<br/>_mysql_s3_acode]
    end
    
    subgraph "üìä Downstream Consumption"
        G[fa:fa-chart-bar Analytics & BI<br/>Dashboards Executivos]
        H[fa:fa-table Silver Layer<br/>Dados Limpos e Transformados]
    end
    
    A --> A1
    A --> A2
    A1 --> B
    A2 --> B
    B --> C
    C --> D
    D --> E
    E --> E1
    E --> E2
    F -.->|Triggers| C
    E1 --> H
    E2 --> H
    H --> G
    
    style A fill:#ff6b6b
    style E fill:#4ecdc4
    style F fill:#45b7d1
    style G fill:#96ceb4
```

## üìä Detalhamento por Etapa

### **1. Origem dos Dados (MySQL ACODE)**

#### Servidor de Produ√ß√£o
```yaml
Configura√ß√£o do Servidor:
  Host: db-hsp-farmarcas.acode.com.br
  Port: 3306
  Database: acode_farmarcas
  User: userfarmarcasac02
  SSL: Enabled (preferred mode)
  Connection Type: Direct (external partner)
  Partner: ACODE - Sistema de gest√£o farmac√™utica
```

#### Tabelas Principais Sincronizadas
| Tabela | Descri√ß√£o | Records (aprox.) | Sync Mode | Primary Key | Criticidade |
|--------|-----------|------------------|-----------|-------------|-------------|
| `farmarcas_si_analitico_diario` | Transa√ß√µes fiscais detalhadas com dados de vendas | ~50M+ | Full Refresh | `idpk` | üî¥ Cr√≠tica |
| `farmarcas_si_analitico_diario_produtos` | Cat√°logo completo de produtos farmac√™uticos | ~500K | Full Refresh | `idpk` | üü° Alta |

#### Schema das Tabelas

**farmarcas_si_analitico_diario** (Dados Fiscais):
```json
{
  "properties": {
    "idpk": {"type": "number", "airbyte_type": "integer", "description": "Chave prim√°ria √∫nica"},
    "ACODE_Val_Total": {"type": "number", "description": "Valor total da transa√ß√£o"},
    "CNPJ": {"type": "string", "description": "CNPJ da farm√°cia compradora"},
    "CNPJ_Fornecedor": {"type": "string", "description": "CNPJ do fornecedor"},
    "Data": {"type": "string", "format": "date", "description": "Data da transa√ß√£o"},
    "Data_Processamento": {"type": "string", "format": "date", "description": "Data de processamento ACODE"},
    "EAN": {"type": "string", "description": "C√≥digo de barras do produto"},
    "Fornecedor": {"type": "string", "description": "Nome do fornecedor"},
    "NF_Numero": {"type": "number", "airbyte_type": "integer", "description": "N√∫mero da nota fiscal"},
    "Qtd_Trib": {"type": "number", "description": "Quantidade tribut√°vel"},
    "Val_Prod": {"type": "number", "description": "Valor do produto"},
    "Val_Trib": {"type": "number", "description": "Valor tribut√°rio"},
    "CFOP": {"type": "number", "airbyte_type": "integer", "description": "C√≥digo Fiscal de Opera√ß√£o"},
    "NCM": {"type": "number", "airbyte_type": "integer", "description": "Nomenclatura Comum do Mercosul"},
    "CST": {"type": "string", "description": "C√≥digo de Situa√ß√£o Tribut√°ria"},
    "Aliquota_ICMS": {"type": "number", "description": "Al√≠quota do ICMS"},
    "Valor_ICMS": {"type": "number", "description": "Valor do ICMS"},
    "IPI": {"type": "number", "description": "Valor do IPI"},
    "ST": {"type": "number", "description": "Substitui√ß√£o Tribut√°ria"},
    "STRet": {"type": "number", "description": "ST Retido"}
  }
}
```

**farmarcas_si_analitico_diario_produtos** (Cat√°logo de Produtos):
```json
{
  "properties": {
    "idpk": {"type": "number", "airbyte_type": "integer", "description": "Chave prim√°ria √∫nica"},
    "EAN": {"type": "string", "description": "C√≥digo de barras do produto"},
    "Produto": {"type": "string", "description": "Nome do produto"},
    "Fabricante": {"type": "string", "description": "Empresa fabricante"},
    "P_Ativo": {"type": "string", "description": "Princ√≠pio ativo"},
    "Classe": {"type": "string", "description": "Classe terap√™utica"},
    "Sub_Classe": {"type": "string", "description": "Sub-classe terap√™utica"},
    "Familia": {"type": "string", "description": "Fam√≠lia do produto"},
    "Grupo": {"type": "string", "description": "Grupo do produto"},
    "Tipo": {"type": "string", "description": "Tipo do produto"},
    "Desc_Marca": {"type": "string", "description": "Descri√ß√£o da marca"},
    "Holding": {"type": "string", "description": "Grupo empresarial"}
  }
}
```

### **2. Processamento via Airbyte**

#### Configura√ß√£o da Source
```yaml
Connector: airbyte/source-mysql v1.0.21
Configuration:
  host: db-hsp-farmarcas.acode.com.br
  port: 3306
  database: acode_farmarcas
  username: userfarmarcasac02
  password: ${ACODE_PASS}  # Vari√°vel de ambiente
  ssl: true
  ssl_mode: preferred
  replication_method: STANDARD (Full Refresh)
  tunnel_method: NO_TUNNEL
```

#### Configura√ß√£o da Destination
```yaml
Connector: airbyte/destination-s3 v0.3.23
Configuration:
  s3_bucket_name: farmarcas-production-bronze
  s3_bucket_region: us-east-2
  s3_bucket_path: "origin=airbyte/database=bronze_acode"
  s3_path_format: "${STREAM_NAME}/cog_dt_ingestion=${YEAR}-${MONTH}-${DAY}/file_${STREAM_NAME}"
  format: Parquet
  compression_codec: SNAPPY
```

#### Resource Allocation
```yaml
Recursos de Processamento:
  CPU Limit: 2.0 cores
  CPU Request: 1.0 core
  Memory Limit: 4Gi
  Memory Request: 2Gi
  Max Active Runs: 1
  Timeout: 3 horas
```

### **3. Armazenamento S3**

#### Estrutura de Diret√≥rios
```
s3://farmarcas-production-bronze/origin=airbyte/database=bronze_acode/
‚îú‚îÄ‚îÄ farmarcas_si_analitico_diario/
‚îÇ   ‚îî‚îÄ‚îÄ cog_dt_ingestion=2025-08-07/
‚îÇ       ‚îú‚îÄ‚îÄ file_farmarcas_si_analitico_diario_20250807_001.parquet
‚îÇ       ‚îú‚îÄ‚îÄ file_farmarcas_si_analitico_diario_20250807_002.parquet
‚îÇ       ‚îî‚îÄ‚îÄ file_farmarcas_si_analitico_diario_20250807_003.parquet
‚îî‚îÄ‚îÄ farmarcas_si_analitico_diario_produtos/
    ‚îî‚îÄ‚îÄ cog_dt_ingestion=2025-08-07/
        ‚îî‚îÄ‚îÄ file_farmarcas_si_analitico_diario_produtos_20250807_001.parquet
```

#### Padr√£o de Nomenclatura
- **Path Format**: `${STREAM_NAME}/cog_dt_ingestion=${YEAR}-${MONTH}-${DAY}/file_${STREAM_NAME}`
- **Particionamento**: Por data de ingest√£o (YYYY-MM-DD)
- **Compress√£o**: SNAPPY para performance otimizada
- **Formato**: Parquet para compatibilidade com analytics

#### Propriedades dos Arquivos
```yaml
Formato Parquet:
  Compress√£o: SNAPPY
  Schema: Preservado do MySQL
  Metadata: Inclu√≠do
  
Tamanho T√≠pico por Arquivo:
  farmarcas_si_analitico_diario: 50-200MB por arquivo
  farmarcas_si_analitico_diario_produtos: 5-20MB por arquivo
  
Particionamento:
  Campo: cog_dt_ingestion (data de ingest√£o)
  Granularidade: Di√°ria
  Reten√ß√£o: Conforme pol√≠tica de lifecycle S3
```

### **4. Orquestra√ß√£o via Airflow**

#### DAG Principal: `dag_sync_connection_mysql_s3_acode`
```python
# Configura√ß√£o da DAG
DAG_CONFIG = {
    'dag_id': 'dag_sync_connection_mysql_s3_acode',
    'schedule': 'Manual',  # Triggered by upstream DAG
    'owner': 'data-engineering',
    'max_active_runs': 1,
    'retry': 1,
    'retry_delay': timedelta(minutes=15),
    'catchup': False
}

# Tasks principais
TASKS = [
    'trigger_acode_sync',      # AirbyteTriggerSyncOperator
    'wait_acode_sync',         # AirbyteJobSensor
    'validate_data_quality',   # PythonOperator (opcional)
    'update_metadata',         # PythonOperator (opcional)
    'notify_completion'        # SlackOperator
]
```

#### Depend√™ncias e Scheduling
- **Upstream**: DAG orchestrator principal
- **Trigger**: Manual via Airflow API ou UI
- **Frequ√™ncia Esperada**: Di√°ria (tipicamente entre 02:00-06:00 UTC)
- **SLA**: 2 horas para completar sincroniza√ß√£o
- **Timeout**: 3 horas m√°ximo

#### Monitoramento da DAG
```python
# M√©tricas coletadas durante execu√ß√£o
EXECUTION_METRICS = {
    'start_time': 'Timestamp de in√≠cio',
    'end_time': 'Timestamp de fim',
    'duration_minutes': 'Dura√ß√£o total em minutos',
    'records_processed': {
        'farmarcas_si_analitico_diario': 'Quantidade de registros',
        'farmarcas_si_analitico_diario_produtos': 'Quantidade de registros'
    },
    'data_size_gb': 'Tamanho total dos dados processados',
    'success_rate': 'Taxa de sucesso (%)',
    'error_count': 'Quantidade de erros'
}
```

## ‚è±Ô∏è Timeline e Performance

### **Tempo de Execu√ß√£o T√≠pico**
```mermaid
gantt
    title Timeline de Execu√ß√£o ACODE
    dateFormat  HH:mm
    axisFormat %H:%M
    
    section Prepara√ß√£o
    Verifica√ß√£o de conectividade    :active, prep1, 00:00, 00:02
    Autentica√ß√£o MySQL             :prep2, after prep1, 00:01
    
    section Extra√ß√£o
    farmarcas_si_analitico_diario   :active, extract1, after prep2, 01:30
    farmarcas_si_analitico_diario_produtos :extract2, after prep2, 00:15
    
    section Upload S3
    Upload arquivos Parquet        :upload1, after extract1, 00:20
    Verifica√ß√£o integridade        :verify1, after upload1, 00:05
    
    section Finaliza√ß√£o
    Cleanup e notifica√ß√µes         :cleanup1, after verify1, 00:02
```

### **M√©tricas de Performance Detalhadas**

#### Volume de Dados por Sync
```yaml
farmarcas_si_analitico_diario:
  registros_por_sync: "1M - 5M (depende do per√≠odo)"
  tamanho_dados: "2-5GB"
  arquivos_gerados: "10-20 arquivos Parquet"
  tempo_processamento: "60-90 minutos"

farmarcas_si_analitico_diario_produtos:
  registros_por_sync: "500K - 800K"
  tamanho_dados: "50-100MB"
  arquivos_gerados: "1-2 arquivos Parquet"
  tempo_processamento: "10-15 minutos"

totais_por_execucao:
  dados_totais: "4-8GB"
  tempo_total: "45-120 minutos"
  pico_cpu: "~80% dos 2.0 cores"
  pico_memoria: "~3GB dos 4GB alocados"
  throughput_rede: "~1-2 Gbps"
```

#### Padr√µes de Crescimento
```yaml
crescimento_mensal:
  farmarcas_si_analitico_diario: "+5-10% registros/m√™s"
  farmarcas_si_analitico_diario_produtos: "+2-3% registros/m√™s"
  
projecoes_anuais:
  registros_2025: "~60M farmarcas_si_analitico_diario"
  tamanho_dados_2025: "~100GB/m√™s"
  necessidade_recursos: "Poss√≠vel scale-up em Q4/2025"
```

## üîç Valida√ß√µes e Qualidade

### **Valida√ß√µes Autom√°ticas Durante o Fluxo**

#### 1. Valida√ß√£o de Conectividade
```sql
-- Teste de conectividade b√°sica
SELECT 1 as connectivity_test;

-- Verifica√ß√£o de permiss√µes
SHOW GRANTS FOR CURRENT_USER();

-- Teste de acesso √†s tabelas
SELECT COUNT(*) FROM farmarcas_si_analitico_diario LIMIT 1;
SELECT COUNT(*) FROM farmarcas_si_analitico_diario_produtos LIMIT 1;
```

#### 2. Valida√ß√£o de Schema
```python
# Verifica√ß√£o de schema antes da extra√ß√£o
EXPECTED_COLUMNS = {
    'farmarcas_si_analitico_diario': [
        'idpk', 'ACODE_Val_Total', 'CNPJ', 'Data', 'EAN', 
        'Fornecedor', 'NF_Numero', 'CFOP', 'NCM', 'CST'
    ],
    'farmarcas_si_analitico_diario_produtos': [
        'idpk', 'EAN', 'Produto', 'Fabricante', 'P_Ativo',
        'Classe', 'Sub_Classe', 'Familia', 'Grupo'
    ]
}

def validate_schema(table_name, actual_columns):
    expected = set(EXPECTED_COLUMNS[table_name])
    actual = set(actual_columns)
    
    missing = expected - actual
    extra = actual - expected
    
    return {
        'schema_valid': len(missing) == 0,
        'missing_columns': list(missing),
        'extra_columns': list(extra)
    }
```

#### 3. Valida√ß√£o de Qualidade de Dados
```sql
-- Verifica√ß√£o de completude
SELECT 
    DATE(cog_dt_ingestion) as ingestion_date,
    COUNT(*) as total_records,
    COUNT(DISTINCT CNPJ) as unique_cnpjs,
    COUNT(DISTINCT EAN) as unique_products,
    SUM(CASE WHEN ACODE_Val_Total > 0 THEN 1 ELSE 0 END) as positive_values,
    AVG(ACODE_Val_Total) as avg_transaction_value
FROM farmarcas_si_analitico_diario 
WHERE DATE(cog_dt_ingestion) = CURRENT_DATE()
GROUP BY DATE(cog_dt_ingestion);

-- Verifica√ß√£o de integridade referencial
SELECT 
    'REFERENTIAL_INTEGRITY' as check_type,
    COUNT(DISTINCT d.EAN) as eans_in_diario,
    COUNT(DISTINCT p.EAN) as eans_in_produtos,
    COUNT(DISTINCT d.EAN) - COUNT(DISTINCT p.EAN) as missing_products
FROM farmarcas_si_analitico_diario d
LEFT JOIN farmarcas_si_analitico_diario_produtos p ON d.EAN = p.EAN
WHERE DATE(d.cog_dt_ingestion) = CURRENT_DATE();

-- Verifica√ß√£o de outliers
SELECT 
    'HIGH_VALUE_TRANSACTIONS' as check_type,
    COUNT(*) as outlier_count,
    MAX(ACODE_Val_Total) as max_value,
    AVG(ACODE_Val_Total) as avg_value
FROM farmarcas_si_analitico_diario 
WHERE ACODE_Val_Total > 100000  -- Valores > R$ 100k
    AND DATE(cog_dt_ingestion) = CURRENT_DATE();
```

### **Valida√ß√µes P√≥s-Upload S3**

#### 1. Verifica√ß√£o de Integridade de Arquivos
```python
import boto3
import pandas as pd

def validate_s3_files(bucket, prefix, date):
    """Validar arquivos S3 ap√≥s upload"""
    s3 = boto3.client('s3')
    
    # Listar arquivos do dia
    response = s3.list_objects_v2(
        Bucket=bucket,
        Prefix=f"{prefix}/cog_dt_ingestion={date}/"
    )
    
    files = response.get('Contents', [])
    
    validation_results = {
        'total_files': len(files),
        'total_size_mb': sum(f['Size'] for f in files) / 1024 / 1024,
        'files_by_table': {},
        'data_integrity': {}
    }
    
    # Validar cada tabela
    for table in ['farmarcas_si_analitico_diario', 'farmarcas_si_analitico_diario_produtos']:
        table_files = [f for f in files if table in f['Key']]
        validation_results['files_by_table'][table] = len(table_files)
        
        # Testar leitura de um arquivo
        if table_files:
            try:
                sample_file = table_files[0]['Key']
                df = pd.read_parquet(f"s3://{bucket}/{sample_file}")
                
                validation_results['data_integrity'][table] = {
                    'readable': True,
                    'record_count': len(df),
                    'column_count': len(df.columns),
                    'null_percentage': df.isnull().sum().sum() / (len(df) * len(df.columns))
                }
            except Exception as e:
                validation_results['data_integrity'][table] = {
                    'readable': False,
                    'error': str(e)
                }
    
    return validation_results
```

## üìä Downstream Impact

### **Sistemas que Dependem dos Dados ACODE**

#### Analytics e BI
```yaml
Dashboards Executivos:
  - Dashboard Financeiro: Receitas e faturamento
  - Dashboard Operacional: Volume de vendas e produtos
  - Dashboard Compliance: Relat√≥rios fiscais e auditoria

Sistemas Downstream:
  - Silver Layer: Dados limpos e transformados
  - Gold Layer: Agrega√ß√µes e m√©tricas de neg√≥cio
  - Data Warehouse: Hist√≥rico consolidado
  - ML Pipelines: Modelos de previs√£o e recomenda√ß√£o
```

#### Criticalidade e Depend√™ncias
```mermaid
graph TD
    A[fa:fa-database ACODE Raw Data] --> B[fa:fa-filter Silver Layer Cleaning]
    B --> C[fa:fa-chart-bar Executive Dashboards]
    B --> D[fa:fa-calculator Financial Reconciliation]
    B --> E[fa:fa-cogs Compliance Reports]
    B --> F[fa:fa-brain ML Models]
    
    C --> G[fa:fa-users Business Users]
    D --> H[fa:fa-building Finance Team]
    E --> I[fa:fa-gavel Audit & Legal]
    F --> J[fa:fa-robot Automated Insights]
    
    style A fill:#ff6b6b
    style C fill:#96ceb4
    style D fill:#feca57
    style E fill:#ff9ff3
    style F fill:#54a0ff
```

### **SLAs Downstream**
```yaml
Executive Dashboards:
  data_freshness_sla: "< 4 horas"
  availability_sla: "99.5%"
  
Financial Reconciliation:
  data_freshness_sla: "< 2 horas"  
  availability_sla: "99.9%"
  accuracy_sla: "99.99%"
  
Compliance Reports:
  data_freshness_sla: "< 24 horas"
  availability_sla: "99.9%"
  completeness_sla: "100%"
```

---

## üîó Links Relacionados

- **[README Principal](./README.md)** - Vis√£o geral do sistema
- **[Redund√¢ncia](./redundancia.md)** - Mecanismos de backup e failover
- **[Configura√ß√µes](./configuracoes_exemplo.md)** - YAMLs e setup completo
- **[Troubleshooting](./erros_comuns.md)** - Resolu√ß√£o de problemas

**Pr√≥ximo**: [Redund√¢ncia](./redundancia.md) - Entenda os mecanismos de backup e failover autom√°tico.
